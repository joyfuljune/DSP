# <div align='center'> DTW孤立词语音识别 </div>

#### 目录
- [MFCC倒谱](#mfcc)
- [DTW算法](#dtw)
  - [多帧对比](#multi)
- [实验](#ex)

<a name="mfcc"></a>
DTW（dynamic time warping 动态时间规整）是一种模板匹配算法。当我们想要识别语音的时候，我们可以用模板匹配法进行识别。它的原理就是用测试语音信号不断对比模板库中的模板语音信号，并计算对比后产生的相似度，取相似度最大者为识别结果。本次实验主要研究两个问题，一是如何取得对比数据，二是DTW算法的核心思想。

除了DTW算法之外，常用的语音识别算法还有支持向量机（SVM），矢量量化（VQ），隐马尔可夫模型（HMM），高斯混合模型（GMM）和人工神经网络等。
以下为整个流程的原理框图

![image](https://user-images.githubusercontent.com/88413945/185897806-e70beca6-5e54-4d27-a44e-fcc34bce3a1c.png)  
*图1. 模板匹配法语音识别系统的原理框图*

## MFCC倒谱（Mel Frequency Cepstral Coefficients）

任何自动语音识别(ASR)系统的第一步都是提取特征，即识别音频信号中有助于识别语言内容的成分，并丢弃所有其他带有背景噪音、情绪等信息的成分。

理解语音的要点是，人类产生的声音是由包括舌头、牙齿等在内的声道形状过滤的，也就是上一次实验所提到的源和滤波器模型。如果我们能够确定形状，确定滤波器，我们就可以准确地表示出正在产生的声音。声道的形状体现在短时功率谱的包络中，而 MFCC 的工作就是准确地表示这个包络。

MFCC是一种广泛应用于ASR的特征提取方法，从1980年由Davis提出至今，都是首屈一指的流行算法。为了计算MFCC倒谱，我们需要做到以下5个步骤

- 分帧加窗

我们需要将信号拆分为短时间帧。 这一步背后的基本原理是信号中的频率会随着时间而变化，因此在大多数情况下，对整个信号进行傅里叶变换是没有意义的，因为我们会随着时间的推移失去信号的频率轮廓。 将信号帧化为 20–40 ms 帧。 25ms 是标准的。 这意味着 16kHz 信号的帧长度为 0.025*16000 = 400 个样本。帧步长（帧移）通常约为 10 毫秒（160 个样本），这允许帧有一些重叠。

加窗本质上是用来显着抵消快速傅里叶变换所做出的数据无限的假设并减少光谱泄漏。

- 计算频谱功率谱

我们现在可以对每一帧做一个 N 点 FFT 来计算频谱，也称为短时傅里叶变换 (STFT)，其中 N 通常为 256 或 512，然后计算能量谱。时域信号x[n]分帧加窗后记为 $x_i[n]$ ，对每帧作FFT的结果记为 $X_i[k]$ ，则功率谱的周期图估计公式为 $P_i[k]=\frac{1}{N}|X_i [k]|^2$ 。保留前一半如512点FFT保留257点。

- 应用过滤器组求频谱能量

梅尔间隔滤波器组是一组 20-40 个三角形滤波器，26是标准形式，图示为40组。 实验中为32组。  
![image](https://user-images.githubusercontent.com/88413945/185898266-8216e301-ef77-435f-88e7-6de395449d2d.png)  
*图2. 美尔滤波器组*

我们的滤波器组以 40 个长度为 257 的向量的形式（彩色三角形）出现（假设上一步的 FFT 设置）。 每个向量大多为零（三角形左右两侧），但对于频谱的某个部分非零（三角内区域）。 为了计算滤波器组能量，我们将每个滤波器组与功率谱相乘，然后将系数相加。 执行此操作后，我们会留下 40 个数字，这些数字可以指示每个滤波器组中有多少能量。

![image](https://user-images.githubusercontent.com/88413945/185898390-60cdd66b-fc0c-48fa-81d8-7f71ab49af0d.png)  
*图3. 梅尔滤波器组和分帧信号的乘积。（a）为梅尔信号组，（b）为一帧语音信号的功率谱，（c）第8个滤波器三角，（d）为图（b）乘以图（c）的结果，（e）第20个滤波器三角，（f）为图（b）乘以图（e）的结果* 

- 取对数能量

一旦我们有了滤波器组的能量，我们就取它们的对数。这也受到人类听觉的推动：我们听到声音的响度和能量之间不是正比关系。通常，要将声音的感知音量放大2倍，我们需要投入 8 倍的能量。这意味着如果声音一开始就很大，那么能量的巨大变化听起来可能并没有那么不同。对数操作使我们的能量特征更接近人类实际听到的内容。为什么是对数而不是3次方？对数允许我们使用倒谱平均减法，这是一种通道归一化技术。

对上一步得到的40个能量作对数，得到40个对数率波器组能量。

- 计算DCT

最后一步是计算对数滤波器组能量的 DCT。因为我们的滤波器组都是重叠的，所以滤波器组的能量彼此之间非常相关。 DCT 对能量进行去相关，这意味着对角协方差矩阵可用于对特征进行建模。对40个对数滤波器组能量进行离散余弦变换 (DCT)，得到40个倒谱系数。 对于 ASR，仅保留 40个系数中较低的 1~12或1~13 个，我们认为通过DCT，能量较高的信息都位于低频处。（回忆实验一）。而0阶系数通常被排除在外，因为它代表输入信号的平均对数能量，它只携带很少的说话人特定信息。

$$
x ̂[i,m]=c(m)\sum_{n=0}^{C-1}E[i,n]cos\left(\frac{\left(n+\frac{1}{2}\right)\pi}{C}m\right), m=0,1,\ldots,C-1
$$

其中，  

$$
c_m=\begin{Bmatrix}
\sqrt{\frac{1}{N}} & m=0\\
\sqrt{\frac{2}{N}} & m\ne0
\end{Bmatrix}
$$

C表示MFCC滤波器组数。E[i，m]表示第i帧滤波器组能量。

- 微分和加速度系数

MFCC倒谱系数通常被称为静态特征，因为它们只包含来自给定帧的信息。 可以通过计算倒谱系数的一阶和二阶导数来获得有关信号时间动态的额外信息。 一阶导数称为 delta 系数，二阶导数称为 delta-delta 系数。 Delta 系数说明语速，而 delta-delta 系数提供类似于语音加速的信息。我们将一阶和二阶导数和MFCC倒谱系数合并可以丰富特征向量。

$$d_t=\frac{\sum_{n=1}^{N} {n(c_{t+n}-c_{t-n})}}{2\sum_{n=1}^{N} {n^2}}$$

N一般取值为2。

<a name = 'dtw'></a>
## DTW算法

使用动态时间规整方法进行模板匹配的原因是因为语音信号的差异总是存在，如图所示同一个女声在不同时间下说出的你好的波形，并不总是一致的。我们的语速、口腔形状、心情等等都会影响我们的发音，所以模板匹配不是100%的结果，只是在众多匹配结果里取相似度更高的一个。相似度最高意味着差异度最小，我们喜欢以最小化讨论数据。差异度以模板特征和测试音频特征之间的距离度量。

举例我们现在分别持有一帧模板语音**a**的特征和测试语音**a**的mfcc系数。假设如图中红色和蓝色折线所示。

![image](https://user-images.githubusercontent.com/88413945/185900154-0e06a71e-63f4-484a-af5e-5e14cb1376a6.png)  
*图4. 一帧信号对比*

我们首先将两条折线如下图放置，零点对齐。

![image](https://user-images.githubusercontent.com/88413945/185900369-59c26032-3af0-4ef9-9168-e992797a1b02.png)  

*图5. 一帧信号距离图谱*

图中distance matrix显示了从左下角到右上角，即0点到终点的距离矩阵。浅绿色标注的路径是最短路径。其计算方式如下所示

![image](https://user-images.githubusercontent.com/88413945/185900463-7061d6e5-4944-44f3-ad23-2f0d180efcea.png)  
*图6. DCT算法计算距离矩阵*

A和B折线的值分别由{3，7，5，…, 3，1}和{1，6，2，…, 6，3}给出。首先计算左侧第一列和底部第一行的数值，然后依次计算中间及其他部分的数值，最终得到一张全部标记好的距离矩阵。

![image](https://user-images.githubusercontent.com/88413945/185900565-71b4d5f5-d7f7-4cdf-a69b-26d08e24a039.png)  
*图7. 路径选择对应这两帧信号的匹配*

右上角的值为距离，从右上角开始倒推，总是选择与当前方格临近的最小值，（临近指的是方格上左，左下、下三个方位的方格），直至到达零点。将所有经过的距离点进行相加即可求得两帧特征参数的对齐序号。对于两个信号中的所有帧进行同样的处理可以得到总的距离参数。

对比模板中的所有信号和测试信号相似度，我们可以认为距离最短者即识别结果。

DTW的 优势在于如果我们的语音长度是不一致的，例如下图，我们可以以重叠对准的方式进行距离计算。

![image](https://user-images.githubusercontent.com/88413945/185900728-ab9aad34-a663-457a-9f2b-68ff2d8dbcc1.png)

*图8. 图左欧几里得距离，图右DTW距离*

<a name="multi"></a>
### 多帧对比
由于音频信号进行了分帧，所以我们进行对比的模板和测试信号的倒谱不是一帧而是多帧，在实验中，我们通过run_all_debug分别读取男声和女声发出“啊”的音频，分别得出两个信号所对应的倒谱为36*12（男声倒谱）和36*16（女声倒谱）。我们可以按照上述逻辑分别对男女声中的每一帧进行对比计算距离，但通常，对于这样的多维数据，我们直接计算两者之间的欧几里得距离。以L2范数为例，

- 设有一维数据{1，2，3，4，5}，则1和5的L2范数是 $\sqrt{(5-1)^2}$   
- 设有二维数据{1，2}, {3，4}，则两个点之间的距离是 $\sqrt{(1-3)^2+(2-4)^2}$ 
- 设有三维数据{1，2，3}, {4，5，6}，则两个点之间的距离是 $\sqrt{(1-4)^2+(2-5)^2+(3-6)^2}$ 
- 将男声倒谱36*12理解为36维数据点12个，女声倒谱36*16理解为36维数据16个点，所以可以建立12*16的L2范数矩阵计算每两个36维数据之间的距离。这个距离，对应了上图中的|Ai-Bj|。得到范数矩阵后，我们可以依照逻辑计算距离矩阵D。

![image](https://user-images.githubusercontent.com/88413945/185901117-62483e25-4b43-4b3b-8450-b2dd730505bb.png)  
*图9. 多帧欧几里得算法计算DTW距离*

<a name='ex'></a>
## 实验

本次实验提供  
1.	两个音频文件a_femal_Sound.wav和a_male_Sound.wav  
2.	一个数据文件template.mat，其中包含中文数字0~9的发音信号所对应的mfcc倒谱，倒谱长度为36  
3.	EstimateMFCC.m文件，以音频序列和采样频率作为输入可以对应的倒谱矩阵  
4.	EstimateMFCC_publish文件夹，可以通过Estimate_publish网页查看mfcc倒谱计算步骤和图形  
5.	dtwScore.m文件，以模板和测试语音所对应的倒谱进行距离矩阵的计算  
6.	run_all_debug文件，无需修改，读取男声和女声的“啊”发音文件，计算距离，用于测试  
7.	run_all_debug文件，无需修改，录入音频，对比template中预先提取的数字倒谱，计算距离  

备注：你可能会发现算法的识别效果非常差，这是正常的。你可以自己录入0~9的声音，经过计算后保存在template.mat中，会提升识别效果。



